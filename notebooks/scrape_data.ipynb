{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3751a3",
   "metadata": {},
   "source": [
    "# Scrape rent data from domain website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea194bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in imports\n",
    "import re\n",
    "from json import dump\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# user packages\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38678c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the postcode of VIC\n",
    "df_postcode = pd.read_csv(\"../data/raw/australian_postcodes.csv\")\n",
    "vic_postcode = df_postcode[df_postcode[\"state\"] == \"VIC\"].reset_index(drop=True)\n",
    "vic_postcode = vic_postcode[\"postcode\"]\n",
    "unique_postcode =vic_postcode.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "413892a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postcode: 3000\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "NUM_PROPERTY_PER_PAGE = 20\n",
    "\n",
    "# begin code\n",
    "url_links = []\n",
    "property_metadata = defaultdict(dict)\n",
    "\n",
    "# generate list of urls to visit according to the postcode\n",
    "for postcode in unique_postcode:\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "    url = BASE_URL + f\"/rent/?postcode={postcode}\"\n",
    "    bs_object = BeautifulSoup(requests.get(url, headers=headers).text, \"html.parser\")\n",
    "    print(postcode)\n",
    "    \n",
    "    # get the number of rent property for each postcode\n",
    "    property_info = bs_object \\\n",
    "        .find(\n",
    "            \"h1\",\n",
    "            {\"data-testid\": \"summary\"}\n",
    "        ).find(\"strong\").text\n",
    "    property_num = int(re.findall(\"(\\d+)\\s\", property_info)[0])\n",
    "\n",
    "    # calculate the total number of pages for each postcode\n",
    "    if (property_num > 0) and (property_num < NUM_PROPERTY_PER_PAGE):\n",
    "        num_page = 1\n",
    "    elif (property_num > 0):\n",
    "        if (property_num % NUM_PROPERTY_PER_PAGE == 0):\n",
    "            num_page = property_num // NUM_PROPERTY_PER_PAGE\n",
    "        else:\n",
    "            num_page = property_num // NUM_PROPERTY_PER_PAGE + 1\n",
    "\n",
    "    # do not consider the empty pages\n",
    "    if (property_num > 0):\n",
    "        for page in range(1, num_page + 1):\n",
    "            url_page = url + f\"&page={page}\"\n",
    "            bs_object_sub = BeautifulSoup(requests.get(url_page, headers=headers).text, \"html.parser\")\n",
    "            \n",
    "            # find the unordered list (ul) elements which are the results, then\n",
    "            # find all href (a) tags that are from the base_url website.\n",
    "            index_links = bs_object_sub \\\n",
    "                .find(\n",
    "                    \"ul\",\n",
    "                    {\"data-testid\": \"results\"}\n",
    "                ) \\\n",
    "                .findAll(\n",
    "                    \"a\",\n",
    "                    href=re.compile(f\"{BASE_URL}/*\") # the `*` denotes wildcard any\n",
    "                )\n",
    "\n",
    "            for link in index_links:\n",
    "                # if its a property address, add it to the list\n",
    "                if \"address\" in link[\"class\"]:\n",
    "                    url_links.append(link[\"href\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "25b57c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each url, scrape some basic metadata\n",
    "for property_url in url_links[1:]:\n",
    "    bs_object = BeautifulSoup(requests.get(property_url, headers=headers).text, \"html.parser\")\n",
    "\n",
    "    if bs_object \\\n",
    "        .find(\"h1\", {\"class\": \"css-164r41r\"}) \\\n",
    "        .text == None:\n",
    "        continue\n",
    "\n",
    "    # looks for the header class to get property name\n",
    "    property_metadata[property_url][\"name\"] = bs_object \\\n",
    "        .find(\"h1\", {\"class\": \"css-164r41r\"}) \\\n",
    "        .text\n",
    "\n",
    "    # looks for the div containing a summary title for cost\n",
    "    property_metadata[property_url][\"cost_text\"] = bs_object \\\n",
    "        .find(\"div\", {\"data-testid\": \"listing-details__summary-title\"}) \\\n",
    "        .text\n",
    "\n",
    "    # looks for the div containing the number of bed, bathroom and parking area\n",
    "    property_metadata[property_url][\"features\"] = bs_object \\\n",
    "        .find(\"div\", {\"data-testid\": \"property-features-wrapper\"}) \\\n",
    "        .text\n",
    "\n",
    "    # looks for the div containing the type of property\n",
    "    property_metadata[property_url][\"type\"] = bs_object \\\n",
    "        .find(\"div\", {\"data-testid\": \"listing-summary-property-type\"}) \\\n",
    "        .text\n",
    "    \n",
    "    # looks for the div containing the list of property features\n",
    "    feature_list = bs_object \\\n",
    "        .find(\"div\", {\"id\": \"property-features\"})\n",
    "    \n",
    "    # looks for the div containing the description of property\n",
    "    property_desc = bs_object \\\n",
    "        .find(\"div\", {\"data-testid\": \"listing-details__description\"})\n",
    "\n",
    "    # check the existing of property information\n",
    "    if feature_list!= None and property_desc!=None:\n",
    "        information = feature_list.text + \" \" + property_desc.text\n",
    "    elif feature_list!= None:\n",
    "        information = feature_list.text\n",
    "    elif property_desc!= None:\n",
    "        information = property_desc.text\n",
    "    \n",
    "    # if do not have any information, set the facility in property equal to no\n",
    "    else:\n",
    "        information = \"none\"\n",
    "        property_metadata[property_url][\"furnitured\"] = \"No\"\n",
    "        property_metadata[property_url][\"pool\"] = \"No\"\n",
    "        property_metadata[property_url][\"gym\"] = \"No\"\n",
    "\n",
    "    if information != \"none\":\n",
    "        # check whether have furniture\n",
    "        if \"unfurnished\" in information.lower():\n",
    "            property_metadata[property_url][\"furnitured\"] = \"No\"\n",
    "        elif (\"furnished\" in information.lower()) or (\"furnitured\" in information.lower()):\n",
    "            property_metadata[property_url][\"furnitured\"] = \"Yes\"\n",
    "        else:\n",
    "            property_metadata[property_url][\"furnitured\"] = \"No\"\n",
    "        \n",
    "        # check whether have pool\n",
    "        if \"pool\" in information.lower():\n",
    "            property_metadata[property_url][\"pool\"] = \"Yes\"\n",
    "        else:\n",
    "            property_metadata[property_url][\"pool\"] = \"No\"\n",
    "\n",
    "        # check whether have gym\n",
    "        if \"gym\" in information.lower():\n",
    "            property_metadata[property_url][\"gym\"] = \"Yes\"\n",
    "        else:\n",
    "            property_metadata[property_url][\"gym\"] = \"No\"\n",
    "    \n",
    "    # extract coordinates from the hyperlink provided\n",
    "    property_metadata[property_url][\"coordinates\"] = [\n",
    "        float(coord) for coord in re.findall(\n",
    "            r\"destination=([-\\s,\\d\\.]+)\",\n",
    "            bs_object \\\n",
    "                .find(\n",
    "                    \"a\",\n",
    "                    {\"target\": \"_blank\", \"rel\": \"noopener noreferer\"}\n",
    "                ) \\\n",
    "                .attrs[\"href\"]\n",
    "        )[0].split(\",\")\n",
    "    ]\n",
    "\n",
    "    # get the description of properties\n",
    "    property_metadata[property_url][\"desc\"] = re \\\n",
    "        .sub(r\"<br\\/>\", \"\\n\", str(bs_object.find(\"p\"))) \\\n",
    "        .strip(\"</p >\")\n",
    "\n",
    "# output to property json in data/raw/\n",
    "with open(\"../data/raw/property.json\", \"w\") as f:\n",
    "    dump(property_metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33833e",
   "metadata": {},
   "source": [
    "# Scrape suburb data based on the postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef25700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant \n",
    "URL = \"https://postcodes-australia.com/state-postcodes/vic\"\n",
    "POSTCODE_URL = \"https://postcodes-australia.com/postcodes/\"\n",
    "\n",
    "# initial postcode with suburbs\n",
    "postcode_metadata = defaultdict(dict)\n",
    "\n",
    "bs_object = BeautifulSoup(requests.get(URL, headers=headers).text, \"html.parser\")\n",
    "\n",
    "# get the postcode link containing the postcode number\n",
    "all_postcode_links = bs_object.find(\"ul\", {\"class\": \"pclist\"})\\\n",
    "                        .findAll(\n",
    "                            \"a\",\n",
    "                            href=re.compile(f\"{POSTCODE_URL}/*\")\n",
    "                        )\n",
    "\n",
    "# get the suburbs based on the postcodes\n",
    "all_suburbs = bs_object.find(\"ul\", {\"class\": \"pclist\"}).findAll(\"ul\")\n",
    "\n",
    "# generate the suburb data corresbonding to each postcode\n",
    "for index in range(len(all_postcode_links)):\n",
    "    postcode = int(all_postcode_links[index][\"title\"].split(\",\")[0][-4:])\n",
    "    suburbs = re.findall(\"<li>([\\w\\s]*)<\\/li>\", str(all_suburbs[index]))\n",
    "    postcode_metadata[postcode] = suburbs\n",
    "\n",
    "# output to postcode match suburb json in data/raw/\n",
    "with open('../data/raw/postcode_match_suburb.json', 'w') as f:\n",
    "    dump(postcode_metadata, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "67f1dc6f6f712f7142079021955b91e049abb319dcfdc9eed010dd73dd4d845d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
